{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Modules\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Modules Related to PDF Pre-Processing\n",
    "import fitz\n",
    "import os\n",
    "import pdfplumber\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from fpdf import FPDF\n",
    "import PIL.Image\n",
    "\n",
    "# Modules Related to PDF Processing\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA,StuffDocumentsChain ,LLMChain\n",
    "# Modules Related to SQL Processing \n",
    "import psycopg2\n",
    "import urllib\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import UnstructuredExcelLoader\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show fpdf---->1.7.2\n",
    "# pip show PyPDF2----->3.0.1\n",
    "# pip show statsmodels----->0.13.5\n",
    "# pip show langchain--->0.1.1\n",
    "# print(\"fitz version:\", fitz.__version__)---->1.23.19\n",
    "# print(\"pdfplumber version:\", pdfplumber.__version__)----> 0.10.3\n",
    "# print(\"PIL.Image version:\", PIL.Image.__version__)---->9.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show langchain--->0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modules Related to AzureOpenAI\n",
    "# from langchain.chat_models import AzureChatOpenAI\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# import json\n",
    "# with open(r'C:\\Users\\40019115\\BMW\\03-04-2024\\Azure.json') as f:\n",
    "#     Azure=json.load(f)\n",
    "\n",
    "\n",
    "# # Define AzureOpenAI Model and Embedding Model\n",
    "# chat_model = AzureChatOpenAI(openai_api_base=Azure['api_base'],\n",
    "#                   azure_deployment=Azure['deployment_name'],\n",
    "#                   openai_api_key=Azure['api_key'],\n",
    "#                   openai_api_type=Azure['api_type'],\n",
    "#                   openai_api_version=Azure[\"api_version\"],\n",
    "#                   seed=1234,\n",
    "#                   temperature = 0\n",
    "                  \n",
    "#                   )\n",
    "\n",
    "# embeddings =OpenAIEmbeddings(\n",
    "#     deployment=Azure['embedding_name'],\n",
    "#     model=Azure['embedding_model'],\n",
    "#     openai_api_base=Azure['api_base'],\n",
    "#     openai_api_type=Azure['api_type'],\n",
    "#     openai_api_key=Azure['api_key'],\n",
    "#     chunk_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Gemini API Key\n",
    "GOOGLE_API_KEY = 'AIzaSyBIBaI7Cr-bINi-cRK9BHa2rUMK2MpqONQ'\n",
    "# Define Model and Embedding Model\n",
    "model = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, google_api_key=GOOGLE_API_KEY)\n",
    "chat_model = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\",google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_query_from_excel(query):  \n",
    "    path=os.getcwd()\n",
    "    file_name=\"Logs.xlsx\"\n",
    "    file_path=os.path.join(path,file_name)\n",
    "\n",
    "    loaders = [UnstructuredExcelLoader(file_path)]\n",
    "\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(loader.load())\n",
    "    vectorstore_db = FAISS.from_documents(docs,embeddings)\n",
    "    embeddings_vector = embeddings.embed_query(query)\n",
    "    docs = vectorstore_db.similarity_search_by_vector(embeddings_vector)\n",
    "\n",
    "    # prompt_template = \"\"\"\n",
    "    # You are the best in reading the excel sheet and summarize about the soure,sink ids and extract available and connected states of the sink ids and source ids.\n",
    "    \n",
    "    # if there is no answers in the context just reply \"Answer is not available in the provided context\"\\n\\n\n",
    "\n",
    "    \n",
    "    # Context:\\n {context}?\\n\n",
    "    # Question: \\n{question}\\n \n",
    "    # Answer: \n",
    "    # \"\"\"\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    You are the best in reading the excel sheet and summarize about the soure,sink ids and extract available and connected states of the sink ids and source ids.\n",
    "    If the user asks any kind of summary related question then you should return the answer in a formatted way with in depth answer of everything with the values\n",
    "    if the question is related to connection states - search the excel for keywords like CS_CONNECTING,CS_CONNECTED.\n",
    "    if the question is related to connection IDs - search the keywords 'ConnectionID','MainConn ID','mainConnectionID'\n",
    "    if the question is realted to available and unavailable state od source ans sink ids - then search or consider keywords 'isavail','available','isSinkAvailable()',''not avail''.\n",
    "    if there is no answers in the context just give response \"please refer the logs\"\\n\\n\n",
    " \n",
    "   \n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    context = docs\n",
    "    prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])\n",
    "    qa_retreival = LLMChain(llm=chat_model,prompt=prompt,return_final_only=True)\n",
    "    response = qa_retreival.invoke({\"context\": context, \"question\": query})\n",
    "    response = response[\"text\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(query):\n",
    "    loaders = [UnstructuredExcelLoader(\"Combined.xlsx\")]\n",
    "    docs = []\n",
    "    for loader in loaders:\n",
    "        docs.extend(loader.load())\n",
    "    vectorstore_db = FAISS.from_documents(docs,embeddings)\n",
    "    embeddings_vector = embeddings.embed_query(query)\n",
    "    docs = vectorstore_db.similarity_search_by_vector(embeddings_vector)\n",
    "    prompt_template = \"\"\"\n",
    "    You are the best in reading the excel sheet give descriptions of sink ids and source ids.\n",
    "    if there is no answers in the context just reply \"Answer is not available in the provided context\"\\n\\n\n",
    "\n",
    "    \n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n \n",
    "    Answer: \n",
    "    \"\"\"\n",
    "    context = docs[:2]\n",
    "    prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])\n",
    "    qa_retreival = LLMChain(llm=chat_model,prompt=prompt)\n",
    "    response = qa_retreival.invoke({\"context\": context, \"question\": query})\n",
    "    response = response[\"text\"]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_agent(query):\n",
    "    prompt = (\n",
    "        \"\"\"\n",
    "            if the query contains the word \"DESCRIPTION\" or \"desc\"\n",
    "            for example: what is the description of id = 647? or \"what is id=647\"\n",
    "            return 'description'; \n",
    "            else:\n",
    "            return 'status'. \n",
    "            Below is the query.\n",
    "            Query: \n",
    "            \"\"\" +\n",
    "       \n",
    "         query\n",
    "    )\n",
    "    # model = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, google_api_key=GOOGLE_API_KEY)\n",
    "    response = chat_model.invoke(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query = \"summarize the sink and souce ids available in the data along with their connection status in a list ?\"\n",
    "# # query = \"list the connection details of sink id 1024?\"\n",
    "# # query = \"summarize the sink and souce ids along with their connection status in a list ?\"\n",
    "# # query = \"summarize the sink and souce ids with their unavailable status in a list ?\"\n",
    "# # query = \"summarize the logs?\"\n",
    "# # query = \"details of sink and souce ids which are not in connected state?\"\n",
    "# # query = \"maximum and minimum connection ids of sink id 646?\"\n",
    "# # query = \"count of connection ids of sink id 1024?\"\n",
    "# # query = \"what is the description of id = 129? \"\n",
    "# # query = \"connections of all the source and sink ids along with connectionID? \"\n",
    "# # query = \"count of unavailable connection IDs made to each source and sink id?\"\n",
    "# # query = \" for the unavailable sink IDs what is the count of connection ID ?\"\n",
    "# # query = \" What is the connection status along with source and sink ids for the different connetions?\"\n",
    "# # query = \" what is the connection status along with source and sink ids?\"\n",
    "# # query = \"list all the connection IDs \"\n",
    "# # query = \"What sources and sink ids are unavailable?\"\n",
    "# # query = \"What are the lastheardsourceandsinkids?\"\n",
    "# # query = \"What are sources and sinks which are unavailable for Connection ID 118?\"\n",
    "# # query = \"What is the maximum and minimum connection ids for source ID 1024?\"\n",
    "# query = \"What sources and sink ids are connected?\"\n",
    "\n",
    "\n",
    "# question_source = query_agent(query) \n",
    "# pattern ='status'\n",
    "# match = re.search(pattern, str(question_source), re.IGNORECASE)\n",
    "# if match:\n",
    "#     print(get_sql_query_from_excel(query))\n",
    "    \n",
    "# else:\n",
    "#     print(get_description(query))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the provided context, there are some instances where sources and sink ids are reported as not available. These instances are indicated by the keywords 'not avail', 'isSinkAvailable', and 'isSourceAvailable'. The following source and sink ids are reported as unavailable:\n",
    "# - MainConn ID=72::isSinkAvailable(): Sink 1024 not avail\n",
    "# - SM MainConn ID=72::isSinkAvailable(): Sink 1024 not avail\n",
    "# - SM MainConn ID=73::isSinkAvailable(): Sink 1024 not avail\n",
    "# - SM MainConn ID=118::isSourceAvailable(): src 644 not avail\n",
    "# - SM MainConn ID=118::isSourceAvailable(): src 647 not avail\n",
    "# - SM MainConn ID=118::isSourceAvailable(): src 646 not avail\n",
    "\n",
    "# Therefore, the unavailable source ids are 644, 647, and 646, and the unavailable sink ids are 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMW\n",
    " \n",
    "# Summarize the logs with answers to the below queries:-\n",
    "# 1. What sources and sink ids are unavailable?\n",
    "# 2. What is the connection status along with source and sink ids for the different connetions?\n",
    "# 3. What are the lastheardsourceandsinkids?\n",
    "# 4. What are sources and sinks which are unavailable for Connection ID?\n",
    "# 5. What is the maximum and minimum connection id?\n",
    " \n",
    "# Remove \"I do not know the answer from the result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like the given template --- 'Connection ID 37: Source ID 129, Sink ID 387\n",
    "#     - Connection ID 38: Source ID 129, Sink ID 404, along with- which source id is connected to what sink id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: if the user says 'summarize the logs or data?' \n",
    "#     the response should include connection id source id, sink id \n",
    "\n",
    "#     there can be other question related to logs. so give appropriate response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMW\n",
    " \n",
    "# Summarize the logs with answers to the below queries:-\n",
    "# 1. What sources and sink ids are unavailable?\n",
    "# 2. What is the connection status along with source and sink ids for the different connetions?\n",
    "# 3. What are the lastheardsourceandsinkids?\n",
    "# 4. What are sources and sinks which are unavailable for Connection ID?\n",
    "# 5. What is the maximum and minimum connection id?\n",
    " \n",
    "# Remove \"I do not know the answer from the result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, request, jsonify\n",
    "# from flask_cors import CORS\n",
    "# import warnings\n",
    "# app = Flask(__name__)\n",
    "# CORS(app)\n",
    "# @app.route('/receive_data_final_final2', methods=['POST'])\n",
    "# def receive_data():\n",
    "#     try:\n",
    "#         if request.method == 'POST':\n",
    "#             received_data = request.json  # Get the JSON data sent from frontend\n",
    "#             print(\"received_data----->\", received_data)\n",
    "            \n",
    "#             # Print received data for testing purposes\n",
    "#             print(\"received_data sample\", received_data[\"messages\"])  # Print received data for testing purposes\n",
    "#             user_input = [data['content'] for data in received_data[\"messages\"] if data.get('role') == 'user']\n",
    "#             print(\"user input----->\", user_input)\n",
    "        \n",
    "#             user_question = user_input[-1]\n",
    "#             print(\"user question\",user_question)\n",
    "\n",
    "#             question_source = query_agent(user_question)\n",
    "#             pattern = 'status'\n",
    "#             match = re.search(pattern, str(question_source), re.IGNORECASE)\n",
    "#             if match:\n",
    "                \n",
    "#                 agent_output=get_sql_query_from_excel(user_question)\n",
    "#                 print(\"output--->\",agent_output)\n",
    "#                 received_data.update({'role': 'assistant', 'content': agent_output})\n",
    "#             else:\n",
    "#                 print(\"nothing\")\n",
    "#                 result=get_description(user_question)\n",
    "        \n",
    "#                 print(\"final result->\",result)\n",
    "    \n",
    "\n",
    "#                 received_data.update({'role': 'assistant', 'content': result})\n",
    "#             return jsonify(received_data)\n",
    "\n",
    "\n",
    " \n",
    "#     except Exception as e:\n",
    "#         print(\"Error:\", e)\n",
    "#         print(\"received_data final ----->\", received_data)\n",
    "#         return jsonify({\"message\": \"Error processing data\"}), 500\n",
    " \n",
    " \n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host='127.0.0.1', port=9009, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:9008\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [29/Jul/2024 13:52:20] \"OPTIONS /receive_data_final_final2 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received_data-----> {'message': 'What is the connection status along with source and sink ids for the different connetions in\\xa0 bullet points?'}\n",
      "received_data sample What is the connection status along with source and sink ids for the different connetions in  bullet points?\n",
      "user question What is the connection status along with source and sink ids for the different connetions in  bullet points?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\40019115\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "127.0.0.1 - - [29/Jul/2024 13:52:37] \"POST /receive_data_final_final2 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output---> - Connection ID 70: Source ID 129, Sink ID 387, State: CS_CONNECTED \n",
      "- Connection ID 71: Source ID 129, Sink ID 404, State: CS_CONNECTING \n",
      "- Connection ID 72: Source ID 1, Sink ID 1024, State: CS_CONNECTING \n",
      "- Connection ID 73: Source ID 3, Sink ID 1024, State: CS_CONNECTING\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import warnings\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "@app.route('/receive_data_final_final2', methods=['POST'])\n",
    "def receive_data():\n",
    "    try:\n",
    "        if request.method == 'POST':\n",
    "            received_data = request.json  # Get the JSON data sent from frontend\n",
    "            print(\"received_data----->\", received_data)\n",
    "            \n",
    "            # Print received data for testing purposes\n",
    "#             print(\"received_data sample\", received_data[\"messages\"])  # Print received data for testing purposes\n",
    "#             user_input = [data['content'] for data in received_data[\"messages\"] if data.get('role') == 'user']\n",
    "            \n",
    "            print(\"received_data sample\", received_data[\"message\"])  # Print received data for testing purposes\n",
    "#             user_input = [data['content'] for data in received_data[\"message\"] if data.get('role') == 'user']\n",
    "#             print(\"user input----->\", user_input)\n",
    "        \n",
    "            user_question = received_data[\"message\"]\n",
    "            print(\"user question\",user_question)\n",
    "\n",
    "            question_source = query_agent(user_question)\n",
    "            pattern = 'status'\n",
    "            match = re.search(pattern, str(question_source), re.IGNORECASE)\n",
    "            if match:\n",
    "                \n",
    "                agent_output=get_sql_query_from_excel(user_question)\n",
    "                print(\"output--->\",agent_output)\n",
    "                received_data.update({'role': 'assistant', 'content': agent_output})\n",
    "            else:\n",
    "                print(\"nothing\")\n",
    "                result=get_description(user_question)\n",
    "        \n",
    "                print(\"final result->\",result)\n",
    "    \n",
    "\n",
    "                received_data.update({'role': 'assistant', 'content2': result,'error':\"error\"})\n",
    "            return jsonify(received_data)\n",
    "\n",
    "\n",
    " \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"received_data final ----->\", received_data)\n",
    "        return jsonify({\"message\": \"Error processing data\"}), 500\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='127.0.0.1', port=9008, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
